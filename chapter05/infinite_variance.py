"""
Off-Policy Evaluation via Importance Sampling

Environment:
- The agent has two actions: BACK (continue) or END (terminate).
- The behavior policy selects actions randomly (with equal probability).
- The target policy always selects BACK.
- The episode ends with a reward of 0 if END is chosen.
- With 90% probability, continuing (BACK) leads to another step; with 10%, the episode ends with reward 1.

Goal:
- Estimate the expected return under the target policy using episodes generated by the behavior policy.
- Apply ordinary importance sampling to estimate the value.

Generates:
- A plot comparing multiple runs of off-policy estimates using ordinary importance sampling.
"""

import numpy as np
import matplotlib

matplotlib.use("Agg")  # Use non-interactive backend
import matplotlib.pyplot as plt

# Actions
ACTION_BACK = 0  # Continue episode
ACTION_END = 1  # Terminate episode


def behavior_policy():
    """
    Behavior policy used to generate trajectories.
    Selects ACTION_BACK or ACTION_END with equal probability.

    Returns:
        int: Chosen action (0 or 1)
    """
    return np.random.binomial(1, 0.5)


def target_policy():
    """
    Target policy the agent is learning to evaluate.
    Always selects ACTION_BACK.

    Returns:
        int: Chosen action (always 0)
    """
    return ACTION_BACK


def play():
    """
    Simulates a single episode based on the behavior policy.

    Returns:
        tuple:
            reward (int): 1 if episode ends with success, otherwise 0
            trajectory (list): list of actions taken
    """
    trajectory = []
    while True:
        action = behavior_policy()
        trajectory.append(action)

        if action == ACTION_END:
            return 0, trajectory  # episode ends with no reward

        if np.random.binomial(1, 0.9) == 0:  # 10% chance of reward
            return 1, trajectory  # episode ends with reward


def figure_5_4():
    """
    Generates Figure 5.4: Off-policy evaluation using ordinary importance sampling.

    - Runs multiple episodes under the behavior policy.
    - Computes weighted returns according to importance sampling.
    - Averages over multiple runs to observe variance in estimation.

    Saves:
        Plot to '../images/figure_5_4.png'
    """
    runs = 10
    episodes = 100000

    for run in range(runs):
        rewards = []

        for episode in range(episodes):
            reward, trajectory = play()

            # Compute importance sampling ratio
            if trajectory[-1] == ACTION_END:
                rho = 0.0  # trajectory not consistent with target policy
            else:
                rho = 1.0 / (
                    0.5 ** len(trajectory)
                )  # each action must have been BACK (0)

            rewards.append(rho * reward)

        # Cumulative average reward estimate
        rewards = np.add.accumulate(rewards)
        estimates = rewards / np.arange(1, episodes + 1)

        plt.plot(estimates)

    plt.xlabel("Episodes (log scale)")
    plt.ylabel("Ordinary Importance Sampling")
    plt.xscale("log")
    plt.savefig("../images/figure_5_4.png")
    plt.close()


if __name__ == "__main__":
    figure_5_4()
